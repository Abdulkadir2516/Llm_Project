Here is the simplified news article:

Social media and other internet platforms will be legally required to block children's access to harmful content from July or face large fines, Ofcom has announced. Tech firms will have to apply the measures by 25 July or risk fines – and in extreme cases being shut down – under the UK's Online Safety Act.

The communications watchdog published more than 40 measures covering sites and apps used by children, ranging from social media to search and gaming. Under the measures, the "riskiest" services must use "highly effective" age checks to identify under-18 users; algorithms must filter out harmful material; all sites and apps must have procedures for taking down dangerous content quickly; and children must have a "straightforward" way to report content.

Melanie Dawes, Ofcom's chief executive, said the changes were a "reset" for children online and that companies failing to act would face enforcement. "They will mean safer social media feeds with less harmful and dangerous content, protections from being contacted by strangers and effective age checks on adult content," she said.

The technology secretary, Peter Kyle, said he was considering a social media curfew for children after TikTok's introduction of a feature that encourages under-16s to switch off the app after 10pm. Online platforms will be required to suppress the spread of harmful content, such as violent, hateful or abusive material and online bullying. More seriously harmful content, including that relating to suicide, self-harm and eating disorders, will need to be kept off children's feeds entirely, as will pornography.