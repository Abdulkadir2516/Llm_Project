{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-community\n",
    "!pip install pypdf\n",
    "!pip install chromadb\n",
    "!pip install langchain\n",
    "!pip install langchain-groq\n",
    "!pip install sentence_transformers\n",
    "!pip install numpy\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA, RefineDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.combine_documents.refine import RefineDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
    "from langchain.chains.llm import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm():\n",
    "    return ChatGroq(\n",
    "        temperature=0,\n",
    "        groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        model_name=\"llama3-70b-8192\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db():\n",
    "    loader = DirectoryLoader(\"./data/\", glob='*.pdf', loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    vector_db = Chroma.from_documents(texts, embeddings, persist_directory='./chroma_db')\n",
    "    vector_db.persist()\n",
    "    return vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yesil\\AppData\\Local\\Temp\\ipykernel_3064\\692452992.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
      "C:\\Users\\yesil\\AppData\\Local\\Temp\\ipykernel_3064\\692452992.py:11: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
      "C:\\Users\\yesil\\AppData\\Local\\Temp\\ipykernel_3064\\692452992.py:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Başlat\n",
    "llm = initialize_llm()\n",
    "db_path = \"./chroma_db/\"\n",
    "if not os.path.exists(db_path):\n",
    "    vector_db = create_vector_db()\n",
    "else:\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
    "\n",
    "retriever = vector_db.as_retriever()\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt'lar\n",
    "first_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a compassionate and knowledgeable AI assistant trained to help individuals dealing with cyberbullying and mental health concerns.\n",
    "Speak with the calm, empathetic, and reassuring tone of a professional psychologist.\n",
    "Your responses should be warm, understanding, and supportive, helping the user feel safe and heard.\n",
    "Provide practical advice, official steps, and online safety tips when relevant.\n",
    "Always remind the user they are not alone and encourage seeking professional help if needed.\n",
    "\n",
    "Rules to follow strictly:\n",
    "\n",
    "1. Use gentle, non-judgmental language that validates the user’s feelings.\n",
    "2. Avoid repeating the same points; keep responses clear, concise, and varied.\n",
    "3. Do not give direct medical diagnoses or prescribe treatments; instead, encourage consulting a qualified professional when necessary.\n",
    "4. Maintain confidentiality and never ask for sensitive personal information.\n",
    "5. Stay away from controversial topics like politics or religion.\n",
    "6. Respond in a calm, soothing manner that helps reduce anxiety.\n",
    "7. Provide emotional support first, then practical advice.\n",
    "8. When sharing official steps or safety tips, present them in an easy-to-understand way.\n",
    "9. Always reassure the user that they are not alone and help is available.\n",
    "\n",
    "Follow these guidelines to create a supportive and safe environment for users.\n",
    "\n",
    "Here is an example of how you should respond:\n",
    "\n",
    "---\n",
    "User: Someone is sharing my private photos online without my permission. What should I do?\n",
    "Chatbot: I'm really sorry you're going through this. First, make sure to take screenshots as evidence. You should report the content to the platform (like Instagram, TikTok, etc.) using their reporting tools. If the content poses a threat or you're underage, also consider informing a trusted adult or contacting local authorities. You're not alone in this — you're doing the right thing by reaching out.\n",
    "---\n",
    "\n",
    "Now, based on the context and your training data, respond to the following query:\n",
    "\n",
    "{context}\n",
    "User: {question}\n",
    "Chatbot:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "normal_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "{context}\n",
    "User: {question}\n",
    "Chatbot:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Bu flag ilk mesajda True olacak\n",
    "first_message = {\"used\": False}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot cevabı\n",
    "def chatbot_response(user_input, history):\n",
    "    if not user_input.strip():\n",
    "        return \"Lütfen geçerli bir mesaj girin.\"\n",
    "\n",
    "    try:\n",
    "        prompt = first_prompt if not first_message[\"used\"] else normal_prompt\n",
    "        first_message[\"used\"] = True  # bir sonraki için artık normal prompt kullan\n",
    "\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",  # 'stuff' yerine bu stuff tüm veriyi birleştirip verir, halüsinasyona açık olur.\n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs={\"prompt\": prompt}\n",
    "        )\n",
    "\n",
    "        response = qa_chain.run(user_input)\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Hata oluştu: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain type Refine chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Refine prompt'ları\n",
    "question_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a compassionate and knowledgeable AI assistant trained to help individuals dealing with cyberbullying and mental health concerns.\n",
    "Speak with the calm, empathetic, and reassuring tone of a professional psychologist.\n",
    "Your responses should be warm, understanding, and supportive, helping the user feel safe and heard.\n",
    "Provide practical advice, official steps, and online safety tips when relevant.\n",
    "Always remind the user they are not alone and encourage seeking professional help if needed.\n",
    "\n",
    "Rules to follow strictly:\n",
    "\n",
    "1. Use gentle, non-judgmental language that validates the user’s feelings.\n",
    "2. Avoid repeating the same points; keep responses clear, concise, and varied.\n",
    "3. Do not give direct medical diagnoses or prescribe treatments; instead, encourage consulting a qualified professional when necessary.\n",
    "4. Maintain confidentiality and never ask for sensitive personal information.\n",
    "5. Stay away from controversial topics like politics or religion.\n",
    "6. Respond in a calm, soothing manner that helps reduce anxiety.\n",
    "7. Provide emotional support first, then practical advice.\n",
    "8. When sharing official steps or safety tips, present them in an easy-to-understand way.\n",
    "9. Always reassure the user that they are not alone and help is available.\n",
    "10. Use the context provided to answer the question, but do NOT copy the context verbatim.\n",
    "11. Generate original, helpful, and empathetic responses based on the user’s question and the context.\n",
    "\n",
    "Follow these guidelines to create a supportive and safe environment for users.\n",
    "\n",
    "Use the following context to answer the question:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "refine_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\", \"existing_answer\"],\n",
    "    template=\"\"\"\n",
    "We have an existing answer based on earlier context:\n",
    "{existing_answer}\n",
    "\n",
    "Now, with the new context below, improve or expand the answer if needed.\n",
    "If the context isn't helpful, keep the existing answer.\n",
    "\n",
    "New Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Refined Answer:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(user_input, history):\n",
    "    if not user_input.strip():\n",
    "        return \"Lütfen geçerli bir mesaj girin.\"\n",
    "\n",
    "    try:\n",
    "        # refine chain_type doğru parametrelerle kuruldu\n",
    "                \n",
    "        initial_chain = LLMChain(llm=llm, prompt=question_prompt)\n",
    "        refine_chain = LLMChain(llm=llm, prompt=refine_prompt)\n",
    "\n",
    "        # Refine chain\n",
    "        combine_docs_chain = RefineDocumentsChain(\n",
    "            initial_llm_chain=initial_chain,\n",
    "            refine_llm_chain=refine_chain,\n",
    "            document_variable_name=\"context\",\n",
    "            initial_response_name=\"existing_answer\" \n",
    "        )\n",
    "\n",
    "        # Retrieval QA zinciri\n",
    "        qa_chain = RetrievalQA(\n",
    "            retriever=retriever,\n",
    "            combine_documents_chain=combine_docs_chain,\n",
    "            return_source_documents=False\n",
    "        )\n",
    "\n",
    "        response = qa_chain.run(user_input)\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Hata oluştu: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Map prompt (her doküman için)\n",
    "map_chain = LLMChain(llm=llm, prompt=question_prompt)\n",
    "\n",
    "# Reduce prompt (tüm map çıktılarının birleşimi)\n",
    "reduce_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"\"\"\n",
    "Tüm önceki yanıtları göz önünde bulundurarak aşağıdaki bilgiyi tek ve tutarlı şekilde cevapla:\n",
    "\n",
    "{text}\n",
    "\n",
    "Cevap:\"\"\"\n",
    ")\n",
    "\n",
    "reduce_llm_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# StuffDocumentsChain ile reduce_llm_chain sarmalanır\n",
    "reduce_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_llm_chain,\n",
    "    document_variable_name=\"text\"\n",
    ")\n",
    "\n",
    "# MapReduce zinciri oluştur\n",
    "combine_docs_chain = MapReduceDocumentsChain(\n",
    "    llm_chain=map_chain,\n",
    "    reduce_documents_chain=reduce_chain,\n",
    "    document_variable_name=\"context\"\n",
    ")\n",
    "\n",
    "# Retrieval QA zinciri oluştur\n",
    "qa_chain = RetrievalQA(\n",
    "    retriever=retriever,\n",
    "    combine_documents_chain=combine_docs_chain,\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "def chatbot_response(user_input, history):\n",
    "    if not user_input.strip():\n",
    "        return \"Lütfen geçerli bir mesaj girin.\"\n",
    "\n",
    "    try:\n",
    "        response = qa_chain.run(user_input)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Hata oluştu: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yesil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Gradio CSS\n",
    "css = \"\"\"\n",
    ".gradio-container {\n",
    "    background-image: url('data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...');\n",
    "    background-size: cover;\n",
    "    background-position: center;\n",
    "    background-repeat: no-repeat;\n",
    "    background-attachment: fixed;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Arayüz\n",
    "with gr.Blocks(css=css) as app:\n",
    "    gr.ChatInterface(\n",
    "        fn=chatbot_response,\n",
    "        title=\"Mental Health Chatbot\"\n",
    "    )\n",
    "\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
